---
title: "Research Review: Alpha Go"
author: "Arun K Viswanathan"
date: "Feb 18th 2017"
output:
  pdf_document:
    highlight: zenburn
    toc: no
  html_document:
    theme: united
    toc: yes
---

```{r setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

This report summarizes the work done by Google in the Alpha Go project [1]. 

Go is a game with a `b^d` search space with `b` approximately 250 and `d` approximately 150. Such a search
space is infeasible to  search exhaustively. The Alpha Go team implemented a variety of techniques to get
a 99.8% win rate against other computer programs and defeated the human European Go champion, Fan Hui,
by 5 games to 0 in October 2015.

The two primary techniques are used for search space reduction in Alpha Go, above and beyond minimax and 
alpha-beta pruning. These are:

1. Position evaluation resulting in truncation of search space by replacing subtrees with an 
approximate value function that predicts the outcome of the subtree.

2. Reduction of the breadth of the search space by sampling actions from a probability distribution over
all possible moves in a given position. In particular, Alpha Go uses the Monte Carlo Tree Search (MCTS) algorithm
as an alternative to minimax.

In addition, Alpha Go relies on supervised learning (SL) to predict expert moves. The SL policy network
consisting of 13 layers is trained with over 30 million positions from the KGS Go Server. This gives Alpha Go
an accuracy of about 57% using all input features, which is a substantial improvement over prior work. The
Alpha Go team had to tradeoff between use of a larger network which would improve accuracy at the cost of time.
As a result of the SL, Alpha Go is able to beat the strongest open-source Go program, Pachi14,
a sophisticated Monte Carlo search program 85% of the time.

In estimating the value function, Alpha Go uses a final neural network with reinforcement learning (RL). Alpha Go is
again trained with data from the KGS Go Server. This typically runs into issues with overfitting to the known 
outcomes. To overcome this, Alpha Go uses a self play data set with 30 million unique positions and plays 
against its own RL network. This reduces the mean square eror (MSE) from 0.37 to 0.226 on the training set and 
0.234 on the test set.

To evaluate the strength of Alpha Go, further tournaments were run against commercial and open source Go 
programs including Crazy Stone, Zen, Pachi and Fuego, which all use MTCS algorithms themselves. This 
tournament showed that Alpha Go won about 99.8% of the time, losing just one game out of 495! Further more,
Alpha Go won between 77% and 99% of games with a 4-stone handicap.

Thus Alpha Go has managed to push the ability of AI Go programs well beyond the capabilites previously expected
for Go programs. 

[1] Mastering the game of Go with deep neural networks and tree search - David Silver et al - doi:10.1038/nature16961


